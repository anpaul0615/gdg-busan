{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch4-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 머신러닝의 네가지 분류\n",
    "\n",
    "- 크게는 지도학습, 비지도학습, 자기지도학습\n",
    "- 세미지도학습..?\n",
    "\n",
    "### 지도학습 : \n",
    "- 샘플데이터가 주어지고 결과를 학습시키는~\n",
    "- 거의 모든 어플리케이션에서 사용하는~\n",
    "- 주로 분류와 회귀~\n",
    "\n",
    "### 비지도학습 : \n",
    "- 입력을줬는데 겨로가는 신경쓰지않고~ 자기스스로학습해서 어떻게어떻게 결과를 만드는~\n",
    "- 데이터에 대한 상관관계를 잘 이용하기위해서 사용하는것처럼 보임~\n",
    "- 데이터가 어마어마하게 많을때~ 데이터분석을위한 데이터셋을 만드는~\n",
    "- 지도학습을 더 잘 사용하기 위해 비지도학습을 사용하는..?\n",
    "\n",
    "### 자기지도학습 : \n",
    "- 사람이 만든 레이블을 쓰지않는다..? => 오토인코더..?데이터를 효율적으로 표현하는 방법을 학습하는~ 입력에는 노이즈+데이터 를 주고 출력으로는 노이즈가 제거된 꺠끗한데이터를~~ 오토인코더에서 생성된 타깃은 수정되지 않은 타깃\n",
    "\n",
    "### 강화학습 : \n",
    "- 에이전트를 학습시키는~ 에이전트의 보상을 최대화 하는 행동을 선택하도록~\n",
    "- ex) 스코어가 가장 높도록~~\n",
    "\n",
    "### 분류와 회귀에서 사용되는 용어 정리\n",
    "- 샘플 == 입력 \n",
    "- 예측 == 출력\n",
    "- 등등\n",
    "\n",
    "### 경험공유\n",
    "\n",
    "- 지도학습 예시\n",
    "- 모터속도.. 피처값들을 모아서.. 고장에대한유무.. 마모율.. 마모율에 대한 데이터를 학습.. 데이터에대한 상관관계로 예측 실행..\n",
    "- 가짜구름 데이터 판독.. 기상청.. \n",
    "- 주로 지도학습에 대한 내용 위주..\n",
    "\n",
    "- 비지도학습예시 \n",
    "- 응성인식.. 13차원 음성데이터.. 13차원(피처)은 볼수없으니 비지도학습으로 차원을 줄여서.. \n",
    "- 사람으로 예를들면.. 키와몸무게에 대한 비율을 할당해서 새로운 피처를 뽑아내면 2차원을 1차원으로 줄이는..\n",
    "\n",
    "- 강화학습\n",
    "- 자율주행학습.. 쿠키런학습.. 마리오학습..\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 4.2. 머신러닝 모델 평가\n",
    "\n",
    "- 과대적합..? 동일한데이터로 모델을 평가하지 않는 이유!\n",
    "\n",
    "- 과대적합이란..? 훈련데이터와 다른 데이터가 입력되면 성능이 떨어지는 현황\n",
    "\n",
    "- 머신러닝의 목표는 처음보는 데이터에서도 일관되게 잘 작동되도록하는것임! (즉 과대적합이 없는)\n",
    "\n",
    "\n",
    "### 훈련,검증,테스트세트\n",
    "- 튜닝할때 쓰는 파라미터를 하이퍼파라미터(레이어갯수, 유닛갯수 등)\n",
    "- 튜닝을 계속하면 거기에서도 과대적합 발생함~\n",
    "- 정보누설..? 모델이 처음본데이터에대한 정보를 조금이라도 알게되면 그 데이터는 변별력이 없다는 의미\n",
    "\n",
    "### 데이터가 충분하지 않을때..?\n",
    "- 단순홀드아웃..? 모든 경우에서 사용해야할듯! 데이터가 많건적건 나눠서 훈련/검증 수행햐아함\n",
    "- K겹교차검증..? \n",
    "- 셔플링을 사용한 반복 K겹? => K겹을 랜덤하게 섞어서! 매번 무작위로!! 대신 비용이 많이 발생함\n",
    "- 어떤방법을 사용하든 훈련+검증 데이터 외에 테스트 데이터는 반드시 준비되어야함!\n",
    "- 텐서플로우블로그 교차검증 포스팅 참조! (2017/12/27/반복-교차-검증)\n",
    "  단순K겹은 들쭉날쭉 결과나오는데, 셔플링을하면 결과가 평준화됨\n",
    "\n",
    "### 평가방식 유의사항\n",
    "- 데이터는 대표성이 있어야함! 가능한 치우침이 없이~\n",
    "- 시간의 방향은 셔플링하면 안됨! (시퀀스데이터는 셔플K겹 사용 불가)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 4.3. 데이터 전처리, 특성공학, 특성학습\n",
    "\n",
    "- 데이터 전처리 목적..?\n",
    "  전처리를 안하면 신경망 아웃풋이 안나옴!\n",
    "  원본 데이터를 신경망에 적용하기 쉽도록 만드는것!\n",
    "\n",
    "- 신경망의 입력은 텐서 형태가 좋겠다~~ 이기 때문에~!\n",
    "  즉 일반데이터를 숫자형태의 텐서형태로 변환(인코딩)\n",
    "\n",
    "- 전처리라는 의미는 거의 일반데이터를 벡터화(텐서화) 한다는 의미임\n",
    "\n",
    "\n",
    "### 신경망을 위한 데이터 전처리\n",
    "- 벡터화,.? 텐서의 형태로 입력데이터를 변환하는것!  \n",
    "\n",
    "- 정규화..? \n",
    "  한 특성의 범위가 0~1 사이인데 다른하나는 100~200 이면 문제가 있음\n",
    "  특성의 범위를 유사하게 변환!! (정규화 등)\n",
    "\n",
    "- 누락된 값..?\n",
    "  누락된값의 의미가 있으면 살리고, 없으면 다른방식으로 표현을 변환하거나 삭제\n",
    "\n",
    "\n",
    "### 특성공학\n",
    "- 많은 데이터 전처리와 특성공학기법은 거의 다 특정 도메인에 종속적임!!  \n",
    "  각 특성에 대한 값을 볼수는 있지만 각 특성들의 관계나 특징에대해서는 알수가 없기때문에..!!\n",
    "  즉 데이터에 대한 이해는 특정 도메인에 대한 이해가 반드시 필요함\n",
    "  ex) 적조 데이터에서는 수온이 주요 속성..? 다른 생물학적 요소가 더 중요했음..!\n",
    "  \n",
    "- 시계의 시간을 읽기위한 특성공학 예시\n",
    "  원본 시계이미지 데이터는 이미지인식부터 해야함..! 복잡함..!\n",
    "  더좋은데이터는 그 데이터를 xy 좌표를 데이터로!\n",
    "  더더좋은데이터는 각도를 데이터로~!\n",
    "\n",
    "- 특성을 잘잡으면 딥러닝을 사용하지않아도 됨!\n",
    "\n",
    "- 특성공학의 핵심은 문제를 쉽게만들기 위한것임!\n",
    "\n",
    "- 최근 딥러닝에서는 대부분의 경우에서 특성공학이 필요없는 수준까지 올라갔음!\n",
    "  (그래도 좀더 복잡한 문제로가면 필요함~!)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 4.4. 과대적합과 과소적합\n",
    "\n",
    "- 결론은 과대적합,과소적합 안되는게 좋음!\n",
    "\n",
    "- 머신러닝의 근본적인 목적은 \" 최적화와 일반화의 중간지점을 찾는것 \"\n",
    "- 최적화는..?\n",
    "- 일반화는..?\n",
    "\n",
    "- 초기에는 최적화와 일반화의 의미가 있음\n",
    "\n",
    "- 과소적합이란..?\n",
    "  ex) 요트구입하는 문제에서..? 일단 그사람은 집이있ㅇ다고 가정할수있음. => 과소적합상황에서는 집이있으면 요트를 구입할꺼라고 판단하는 것! (다른 피처를 고려하지않은.. ) (모델이 더 발전할 계기가 있다는 의미로도 해석할수있음)\n",
    "  \n",
    "- 과대적합이란..?\n",
    "  계속 트레이닝세트에 맞춰서 학습하다보니 발생하는~~ 조그만한 요소하나하나에도~~\n",
    "\n",
    "- 과대적합을 어떻게 피하는가?\n",
    "  가장좋은것은 훈련데이터를 많이 모으는것! (불가능한상황도있음.. 주식데이터.. 등)\n",
    "  과대적팝을 피하는걸 규제(reguliaziation) 이라고 함\n",
    "\n",
    "### 네트워크 크기 축소\n",
    "- 학습 파라미터의 수를 모델의 용량이라고함 => 이 파라미터들을 레이어마다 조금씩 줄이는게..?\n",
    "- 항상 유념해야할것은~\n",
    "- 최적화가 중요한게 아니라 트레이닝셋에 대해~ 새로운데이터에대해 얼마나 나오는게 더 중요하단말~\n",
    "- 너무과해서도 안되고 너무작아서도 안되고..\n",
    "- 코드 + 결과이미지 보면서 비교~\n",
    "\n",
    "\n",
    "### 가중치 규제 추가\n",
    "- 각각의 특징들마다 가중치가 있을텐데, 각 피처마다 가중치를 얼마나 가질수있게 할것인가 에 대한 조정이 필요~\n",
    "- 같은 설명이 두개일때 더 적은 가정이 필요한 간단ㅅ한 설명이 더 좋다는 의미~\n",
    "- 네트워크의 복잡도에 대한 제한을 걸어서~ 가주치를 최대한 작게 해도록 하는~ \n",
    "- L1규제~ L2 규제~\n",
    "- 페널티는 훈련할떄만 추가하면 된다~ (패널티..? 손실함수에 추가로 더해지는 규제)\n",
    "- 패널티가 적용되면, 훈련때의 손실이 테스트때의 손실보다 더 높아지게됨!! (이게핵심이네)\n",
    "\n",
    "\n",
    "### 드롭아웃 추가\n",
    "- 신경망에서 사용되는 규제기법중에 가장효과적이라고함\n",
    "- 훈련하는동안 무작위로 층의 일부 출력 특성을 제외하는 기법 (일부러성능을떨어뜨리고, 그상태에서 계속 학습을 진행시키느느낌..?)\n",
    "- 드롭아웃비율은 0이될 특성의 비율 \n",
    "- 드롭아웃은 왜쓰는가..? 과적합을 막으려고 쓰는거임!!\n",
    "  핵심아이디어는 층의 출력값에 노이즈를 추가하여 중요하지않은 우연한 패턴을 깨뜨리는것임!\n",
    "- 은행에서 사용하는 부정감지 방법에 착안함~\n",
    "  창구의 직원을 순환시켜서, 직원들과 고객들의 유대관계를 없애서 부정을 예방하는~\n",
    "\n",
    "\n",
    "---\n",
    "## 4.5. 보편적인 머신러닝 작업흐름\n",
    "\n",
    "- 그냥 이렇게이렇게 흘러갑니다~ (읽어보면되는~)\n",
    "\n",
    "\n",
    "### 문제정의와 데이터셋 수집\n",
    "- 입력이무엇인가, 어떤데이터를사용할것인가\n",
    "- 입력으로 출력을 예측할수있다고 가설을 세우고~ 가용대에터로 어떻게 입출력간으 관계를 세우는지~\n",
    "- 가장 풀기어려운문제는 시간에 대한 문제~\n",
    "  ex) 계절의 흐름에 따른 옷 구매 패턴 예측 문제 등  (시간에 따라 데이터가 달라지므로)\n",
    "\n",
    "\n",
    "### 성공지표 선택\n",
    "- 성공이 무엇인가를 정의하는것!\n",
    "- 자신만의 지표를 정의하는건 안좋음! 일반적은 지표를 사용해야함! (정확도, 정밀도, 고객재방분율 등)\n",
    "\n",
    "\n",
    "### 평가방법 선택\n",
    "- 홀드아웃, k겹, 반복k폴드 등\n",
    "- 보통은 폴드아웃으로 충반함\n",
    "\n",
    "\n",
    "### 데이터준비\n",
    "- 데이터는 텐서로 구성되어야함 (결국은 전처리를해야한다)\n",
    "\n",
    "\n",
    "### 기본보다 나은 모델기훈련하기\n",
    "- 통계적 검정력을 달성하는것\n",
    "  이를테면, 이전에 만든 모델보다는 좋아야한다는것!\n",
    "- 만약 성능이 향상이 안됬다면, 처음부터 다시해야한다!! ( 가설세우기부터~ 전처리부터~)\n",
    "- 만약 이까지 잘했는데도 문제가있으면..>?\n",
    "  마지막층의 활성화함수 확인 : 출력에 필요한 제한을 가함\n",
    "  손실함수 확인 : 적절한 손실함수인지 점검하고~\n",
    "  최적화설정 : 어떤옵티마이저를 사용하는지 확인~ 학습률확인~\n",
    "- 손실함수의 선택에 대해서.. => 주어진 문제의 성공지표를 직접최적화 안된다는것!\n",
    "  (손실함수에 대한 최소요구사항 확인하고!!)\n",
    "- 모델에 맞는 마지막층의 활성화 함수와 손실함수 선택 제안표 (오오!!)\n",
    "  이미 잘 정리되어있는걸 사용하자!\n",
    "  \n",
    "  \n",
    "### 몸짓키우기 : 과대적합 모델 구축\n",
    "- 일반화 모델을 위해서 일단 과대적합을 시켜보느것~!\n",
    "  경계를 확인하기 위해서~!\n",
    "- 과대적합을 만드는방법~~\n",
    "\n",
    "\n",
    "### 모델규제와 하이퍼파라미터 튜닝\n",
    "- 하이퍼파라미터란..?\n",
    "- 가장좋은모델을 얻을때까지 (앞선단계들을) 반복~\n",
    "- 만족할만한 모델이 나오면 훈련+검증 데이터로 모델 훈련 => 딱한번 테스트데이터로 확인~!\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 4.6. 요약\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오프더레코드\n",
    "\n",
    "\n",
    "### L1, L2 차이..?\n",
    "- 절댓값에 비례하는 비용에따라서..?\n",
    "- L1 : sum( w 절대값 )\n",
    "- L2 : sum( w 절대값의 제곱 )\n",
    "- LN : 같은원리로~\n",
    "- L2 이상에서는 w 가 절대 0이 될수는 없음 (L1에는 있을수있음)\n",
    "\n",
    "\n",
    "### 규제 쓰는 이유..?\n",
    "- 모레주머니 차는것같은~\n",
    "\n",
    "\n",
    "### 드롭아웃과 규제..?\n",
    "- 드롭아웃은 원래그래프를 따라가면서 조금씩만 따라가는느낌인데, 규제는 상승을 거으 안하는느낌~\n",
    "- 데이터셋에 따라 다름~~\n",
    "\n",
    "\n",
    "### 튜닝 과도..?\n",
    "- 하나씩하나씩 경험적으로 데이터에 테스트해보는~ 경험적!\n",
    "- 너무 섞어서쓰거나 하면 결과가 희석되서 나옴!!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
